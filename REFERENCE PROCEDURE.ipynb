{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "Create a summary of what the project goal is, information given, challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET MODULES NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install the python package that has libraries needed\n",
    "# Once they are installed, you do not have to install them again\n",
    "# Ensure in correct directory or will not install properly\n",
    "# in juptyer, ! pip install (then name of package here)\n",
    "# in R, require(packagename)\n",
    "\n",
    "# Second, import libraries needed for the project\n",
    "# in juptyer, import pandas\n",
    "# in R, library(librayname)\n",
    "\n",
    "# If going to use a library frequently then importing under an alias. There are common alias for some libraries.\n",
    "# import pandas as pd\n",
    "\n",
    "# If only going to use a couple libraries from a package then import only those\n",
    "# from sklearn.svm import NuSVR, SVR\n",
    "\n",
    "# if using magic functions, import those. \n",
    "\n",
    "# The following renders the figure in a notebook (instead of displaying a dump of the figure object)\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional calls for general control to data or output\n",
    "\n",
    "# suppresses repeated warnings from the same source to cut down on the annoyance of seeing the same message over and over\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# There are five preset seaborn themes: darkgrid, whitegrid, dark, white, and ticks. \n",
    "# They are each suited to different applications and personal preferences. The default theme is darkgrid.\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# pandas has an options system that lets you customize some aspects of its behaviour, \n",
    "# display-related options being those the user is most likely to adjust\n",
    "# the following displays a maximum of 100 columns\n",
    "# pd.set_option('max_columns', 100)\n",
    "\n",
    "# The following sets color palette. Named palettes default to 6 colors but the following sets it to 10 colors.\n",
    "# Gives a more diverse range for graphs\n",
    "# my_pal = sns.color_palette(n_colors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the SIZE of the dataset (usually train). Look at when not compressed.\n",
    "# 4 GB is quite large. A GB is 1,000,000 KB.\n",
    "# In kaggle, can see the size of the dataset if click on the file.\n",
    "# Otherwise, after downloading, click properties over the file name and can see the size.\n",
    "# If it is very large then must read in only part of it??\n",
    "\n",
    "# Example of loading dataset and assigning it to a variable\n",
    "# Ensure have full location of dataset in reference to the root directory for juptyer or where working from\n",
    "# Setting header to None ensures headers are not imported in\n",
    "# Assign dataset to a variable name\n",
    "# cc_apps = pd.read_csv(\"datasets/cc_approvals.data\", header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSPECT & CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the data\n",
    "# cc_apps.head(n=5)\n",
    "\n",
    "# Summary statistics\n",
    "# cc_apps_describe = cc_apps.describe()\n",
    "# print(cc_apps_description)\n",
    "# print(\"\\n\")\n",
    "# in R, summary(dataset)\n",
    "\n",
    "# Info about the dataframe\n",
    "# cc_apps_info = cc_apps.info()\n",
    "\n",
    "# Head of dataset\n",
    "# cc_apps.head()\n",
    "# Tail of dataset, 17 rows in this case\n",
    "# cc_apps.tail(17)\n",
    "# in R, names(dataset)\n",
    "\n",
    "# Shape of dataset\n",
    "# train.shape\n",
    "\n",
    "# After inspecting the data, merge similar datasets. However, don't merge train and test. Keep test set separate if possible\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\n",
    "# train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "\n",
    "# Determine missing values\n",
    "# train.isnull().any().sum()\n",
    "\n",
    "# Dealing with missing values\n",
    "# Drop column if too many missing values.\n",
    "# inplace=True avoids assignment statement\n",
    "# train.drop('column_name', axis='columns', inplace=True)\n",
    "# dropna() drops row bases on the presence of missing values\n",
    "# dataset.dropna(subset=['col1', 'col2'], inplace=True)\n",
    "# ME: think i would like a table of all the columns and missing values (get code for this)\n",
    "\n",
    "# When you know that a specific column will be critical to your analysis, and only a small fraction of rows are missing \n",
    "# a value in that column, it often makes sense to remove those rows from the dataset.\n",
    "\n",
    "# Ensure the columns have the proper data types\n",
    "# types are: object (python strings, lists), boolean (true and false values), int, float, datetime, category\n",
    "# best to not have data as strings\n",
    "# int - enables mathematical operations\n",
    "# float - enables mathematical operations\n",
    "# datetime - enables date-based attributes and methods\n",
    "# category - uses less memory and runs faster\n",
    "# bool - enable logiccal and mathematical operations\n",
    "\n",
    "# check datatypes as dataset.column.dtype\n",
    "# change datatype where the brackets on left side creates or overwrites existing series\n",
    "# dataset['column_name']=dataset.column.astype('data_type')\n",
    "\n",
    "\n",
    "# Should create scatterplots and box plots to look at data\n",
    "# Scatterplot matrix really good way of plotting and visualizing every variable against every other variable.\n",
    "\n",
    "# Some variables act as surogates for the other . eg ldl vs obesity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine which variables to keep\n",
    "- if use cross validation, must use it to narrow down the variables; NOT just randomly pick\n",
    "- 5 methods of building models: all-in, backward elimination, forward selection, bidirectional elimination, score comparison\n",
    "- All-in use when know they are the true predictors OR you have to use all OR preparing for backward elimination\n",
    "- Backward elimination. 1. select significance level to stay in the model (eg 5%); 2. fit model with all predictors; 3. Remove the predictor with highest variable IF it is above the significance level; 4. Fit the model without this variable; 5. Repeat until predictor with highest p value is below significance.\n",
    "- Forward selection. 1. select significance level for model; 2. fit all simple regression models and choose model with lowest p value; 3. Keep this variable and fit all possible models with one extra predictor added to the one(s) you already have; 4. Consider the predictor with the lowest p-value. If above significance, redo step 3; 5. stop when highest predictor is below significance level\n",
    "- Bidirectional elimination. 1. select significance level to stay in model; 2. perform forward selection (new variable must have p<significance to enter); 3. perform all step of backward elimination (old variable must have p<significance to stay); 4. stop when no new variable to enter or add\n",
    "- All possible models/ score comparision. 1. select a criterion of goodness of fit (eg. Akakie criterion); 2. construct all possible regression models; 3. select one with best criterion; 4. model is done\n",
    "- Exceptions: Even though the p-value may be above the signifance level, if it is only marginally above, then look at adjusted R squared of the entire model to see if should keep the variable. If R squared goes up, model is a better fit so keep the variable that is slightly above the significance level.\n",
    "- When referring to the coefficients, ensure referring to units properly (whether dollars, km, etc.). Can compare if on same scale.\n",
    "- Coefficients for each variable change as change the variables in the model.\n",
    "\n",
    "- Best to understand the variables because if just put all the variables in, there is a chance that it may show significance in formula.\n",
    "- for example, things that are not significant may be row number, surname (but may be indicator of something else)\n",
    "\n",
    "TRANSFORM VARIABLES\n",
    "- change transform by scale, square root of variable, apply power to variable, log of variable like 10 (when greater than zero)\n",
    "- can add 1 to variable or split variable into 2\n",
    "- try it out and see if improves model\n",
    "- want to remove the tranformed variable from the model to avoid correleated varaibles\n",
    "- divide one variable by another OR multiply one variable by another eg wealth = balance/age\n",
    "- if create a new variable, remove correlated variables\n",
    "- determine in multicolinearity (which don't want), gretl does automatically, unsure how otherwise. Take suspected colinearity out of the model and the variable becomes significant then likely was colinearity.\n",
    "- can transform the new variables as well\n",
    "- create a correlation matrix to determine correlations of variables where 1 is perfectly correlated and want to avoid this. Avoid over 0.7, correct. 0.5 to 0.3, look into it. Below 0.3 is low correlation. Correlation is bad because it can't find the correct correlation for the variables.\n",
    "- should not just base on accuracy rate since can be accuracy paradox where abandonment of one model increases accuracy rate\n",
    "- cumulative accuracy profile can plot models against each other so can see how much gain get from one over the other. Diagonal line at 45 degrees is the random model. If model is under the curve, very bad model.\n",
    "- to get accuracy ratio, look at % on y axis when take 50% on x axis. If value on y less than 60% than bad. If y value between 60% & 70% then poor. If y value between 70% & 80% then good. If above then very good. If between 90% & 100% then may be too good.\n",
    "\n",
    "\n",
    "\n",
    "HOW DO I DETERMINE SIGNIFICANCE LEVEL\n",
    "explain dummy variables to replace a variable (use one less when fitting the model)\n",
    "good way to understand is visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are they CATEGORICAL (can be nominal like m/f or ordinal like ordered as in small/med/large) \n",
    "# OR NUMERIC (can be discrete like whole numbers or continous like fractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, it isn't good to customize the data. However, if there is not a lot of training data such as was the case with the earthquake competition where there were only 16 training earthquakes, it may be necessary to remove some data in order to ensure the training model is not skewed.\n",
    "In the case of the earthquake file, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earthquake competition (given 16 earthquakes as train and small segments as test):\n",
    "- winner: the signal had a certain time-trend that caused some issues specifically on mean and quantile based features. To partly overcome this, we added a constant noise to each 150k segment (both in train and test) by calculating np.random.normal(0, 0.5, 150_000). Additionally, after noise addition, we subtracted the median of the segment.\n",
    "- i did a bar chart of the most common values. The values were all very small. So i subtracted out a range of the small values.\n",
    "- One of our best final LGB model only used four features: (i) number of peaks of at least support 2 on the denoised signal, (ii) 20% percentile on std of rolling window of size 50, (iii) 4th and (iv) 18th Mel-frequency cepstral coefficients mean. We sometimes used a few more features (like for the NN, see below) but they are usually very similar. Those 4 are decently uncorrelated between themselves, and add good diversity. For each feature we always only considered it if it has a p-value >0.05 on a KS statistic of train vs test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times, people combine several models together to get final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear fit\n",
    "# in R, fit1=lm(medv~lstat, data=Boston)\n",
    "\n",
    "# Multiple linear regression\n",
    "# in R, fit2=lm(medv~lstat+age, data=Boston)\n",
    "# in R to fit all, fit3=lm(med~., Boston)\n",
    "# in R to remove a couple variables, fit4=update(fit3, ~.-age-indus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can get a summary of fit that shows residuals, co-efficients\n",
    "# in R, summary(nameofmodel)\n",
    "# in python, THINK nameof model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER DEFINIED FUNCTIONS\n",
    "- first line put def then the name of the function (square in this case) then () with parameter (value in this case) in the brackets:\n",
    "- second line must be indented then variable name is assigned to the function that want to perform on \"value\" parameter\n",
    "- third line will return the value from function\n",
    "- so when call function, pass argument 4 into function with square(4) and result is 16\n",
    "- def square(value):\n",
    "-     new_value = value ** 2\n",
    "-     return new_value\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
