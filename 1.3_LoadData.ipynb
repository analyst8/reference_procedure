{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data <a class=\"anchor\" id=\"acquire\"></a>\n",
    "\n",
    "Steps are:\n",
    "    - Obtain data\n",
    "        - scrape website\n",
    "        - sql query\n",
    "        - use given data\n",
    "    - Load data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# OR\n",
    "# read from website\n",
    "df = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.text\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ENSURE IN CORRECT WORKING DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory\n",
    "os.getcwd()\n",
    "\n",
    "# change working directory \n",
    "os.chdir('C:\\\\Users\\\\sexto\\\\Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if manually downloading data, can check files in directory to ensure it is there\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make code as reproducible as possible by loading data from the direct link from the web\n",
    "trainUrl = \"https://www.kaggle.com/c/titanic/download/GQf0y8ebHO0C4JXscPPp%2Fversions%2FXkNkvXwqPPVG0Qt3MtQT%2Ffiles%2Ftrain.csv\"\n",
    "df2 = pd.read_csv(trainUrl, sep=',')\n",
    "\n",
    "# However, reading in this file caused problems. I am unsure why? It was supposed to be clean data\n",
    "# Got an answer from stackoverflow from below which is tried dropping rows with errors but decided not to do this\n",
    "train = pd.read_csv(\"https://www.kaggle.com/c/titanic/download/GQf0y8ebHO0C4JXscPPp%2Fversions%2FXkNkvXwqPPVG0Qt3MtQT%2Ffiles%2Ftrain.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Size of data is important\n",
    "- Look at the SIZE of the dataset(s) (usually train). Look at when not compressed.\n",
    "- 4 GB is quite large. A GB is 1,000,000 KB.\n",
    "- In kaggle, click on grid icon and can see the size of the dataset if click on the file.\n",
    "- Otherwise, after downloading, click properties over the file name and can see the size.\n",
    "- If file is very large then must read in only part of it. There is a way to break this up like the earthquake file.\n",
    "- train = train.sample(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different packages have their own way of reading data. \n",
    "- Pandas\n",
    "- Dask\n",
    "- Datatable\n",
    "- Rapids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from methods of reading data from the raw csv files, it is also common to convert the dataset into another format which uses lesser disk space, is smaller in size and/or can be read faster for subsequent reads. The file types explored in the notebook (Default csv and rest alphabetically):\n",
    "- csv\n",
    "- feather\n",
    "- hdf5\n",
    "- jay\n",
    "- parquet\n",
    "- pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that just reading data is not the end of the story. The final decision of which method to use should also consider the downstream tasks and processes of the data that will be required to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PANDAS\n",
    "Pandas is probably the most popular method of reading datasets and is also the default on Kaggle. It has a lot of options, flexibility and functions for reading and processing data.\n",
    "\n",
    "One of the challenges with using pandas for reading large datasets is it's conservative nature while infering data types of the columns of a dataset often resulting in unnecessary large memory usage for the pandas dataframe. You can pre-define optimal data types of the columns (based on prior knowledge or sample inspection) and provide it explicitly while reading the dataset.\n",
    "\n",
    "Documentation: https://pandas.pydata.org/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting header to None ensures headers are not imported in IF you dont want them\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%%time\n",
    "\n",
    "# set names of columns AND data type in the column\n",
    "dtypes = {\n",
    "    \"row_id\": \"int64\",\n",
    "    \"user_id\": \"int32\",\n",
    "    \"task_container_id\": \"int16\",\n",
    "    \"user_answer\": \"int8\",\n",
    "    \"prior_question_elapsed_time\": \"float32\", \n",
    "    \"prior_question_had_explanation\": \"boolean\"\n",
    "}\n",
    "\n",
    "data = pd.read_csv(\"../input/riiid-test-answer-prediction/train.csv\", dtype=dtypes)\n",
    "\n",
    "# look at header to ensure loaded correctly\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORT EXCEL SHEETS WITH PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT EXCEL SHEETS WITH PANDAS\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)\n",
    "\n",
    "# Load a sheet into a DataFrame by name of sheet\n",
    "df1 = xls.parse('2004')\n",
    "\n",
    "# Load a sheet into a DataFrame by index\n",
    "df2 = xls.parse(0)\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column\n",
    "df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DASK\n",
    "Dask provides a framework to scale pandas workflows natively using a parallel processing architecture. For those of you who have used Spark, you will find an uncanny similarity between the two.\n",
    "â€‹\n",
    "Documentation: https://docs.dask.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dtypes = {\n",
    "    \"row_id\": \"int64\",\n",
    "    \"user_id\": \"int32\",\n",
    "    \"task_container_id\": \"int16\",\n",
    "    \"user_answer\": \"int8\",\n",
    "    \"prior_question_elapsed_time\": \"float32\", \n",
    "    \"prior_question_had_explanation\": \"boolean\"\n",
    "}\n",
    "\n",
    "data = dd.read_csv(\"../input/riiid-test-answer-prediction/train.csv\", dtype=dtypes).compute()\n",
    "\n",
    "print(\"Train size:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATATABLE\n",
    "Datatable (heavily inspired by R's data.table) can read large datasets fairly quickly and is often faster than pandas. It is specifically meant for data processing of tabular datasets with emphasis on speed and support for large sized data.\n",
    "\n",
    "Documentation: https://datatable.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datatable installation with internet\n",
    "# !pip install datatable==0.11.0 > /dev/null\n",
    "\n",
    "# datatable installation without internet\n",
    "!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null\n",
    "\n",
    "import datatable as dt\n",
    "\n",
    "%%time\n",
    "\n",
    "data = dt.fread(\"../input/riiid-test-answer-prediction/train.csv\")\n",
    "\n",
    "print(\"Train size:\", data.shape)\n",
    "\n",
    "# read and change to pandas dataframe at same time\n",
    "# data = dt.fread(\"../input/riiid-test-answer-prediction/train.csv\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAPIDS\n",
    "Rapids is a great option to scale data processing on GPUs. With a lot of machine learning modelling moving to GPUs, Rapids enables to build end-to-end data science solutions on one or more GPUs.\n",
    "\n",
    "Documentation: https://docs.rapids.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rapids installation (make sure to turn on GPU)\n",
    "import sys\n",
    "!cp ../input/rapids/rapids.0.15.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n",
    "\n",
    "import cudf\n",
    "\n",
    "%%time\n",
    "\n",
    "data = cudf.read_csv(\"../input/riiid-test-answer-prediction/train.csv\")\n",
    "\n",
    "print(\"Train size:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RELATIONAL DATABASE\n",
    "A relational database is a type of database that stores and provides access to data points that are related to one another. ... The columns of the table hold attributes of the data, and each record usually has a value for each attribute, making it easy to establish the relationships among data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM Album\", engine)\n",
    "\n",
    "# Perform query and save results to DataFrame: df\n",
    "# Save dataframe column names to corresponding names of table columns\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CONVERT DATA TO ANOTHER FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from csv using datatable and converting to pandas\n",
    "data = dt.fread(\"../input/riiid-test-answer-prediction/train.csv\").to_pandas()\n",
    "\n",
    "# writing dataset as csv\n",
    "data.to_csv(\"riiid_train.csv\", index=False)\n",
    "\n",
    "# writing dataset as hdf5\n",
    "# HDF5 is a high-performance data management suite to store, manage and process large and complex data.\n",
    "data.to_hdf(\"riiid_train.h5\", \"riiid_train\")\n",
    "\n",
    "# writing dataset as feather\n",
    "# It is common to store data in feather (binary) format specifically for pandas. \n",
    "# It significantly improves reading speed of datasets.\n",
    "data.to_feather(\"riiid_train.feather\")\n",
    "\n",
    "# writing dataset as parquet\n",
    "# In the Hadoop ecosystem, parquet was popularly used as the primary file format for tabular datasets \n",
    "# and is now extensively used with Spark.\n",
    "data.to_parquet(\"riiid_train.parquet\")\n",
    "\n",
    "# writing dataset as pickle\n",
    "# Python objects can be stored in the form of pickle files and pandas has inbuilt functions\n",
    "# to read and write dataframes as pickle objects.\n",
    "data.to_pickle(\"riiid_train.pkl.gzip\")\n",
    "\n",
    "# writing dataset as jay\n",
    "# Datatable uses .jay (binary) format which makes reading datasets blazing fast. \n",
    "dt.Frame(data).to_jay(\"riiid_train.jay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting stock data from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(2006, 1, 1)\n",
    "end = datetime.datetime(2016, 1, 1)\n",
    "\n",
    "# Bank of America\n",
    "BAC = data.DataReader(\"BAC\", 'google', start, end)\n",
    "\n",
    "# Could also do this for a Panel Object\n",
    "df = data.DataReader(['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC'],'google', start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
