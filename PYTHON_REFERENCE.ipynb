{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate (like first and last name together) in excel\n",
    "convert to other data types\n",
    "ISBLANK in excel\n",
    "maybe list a set of functions for excel here\n",
    "WEEKNUM\n",
    "convert to day of week using WEEKDAY\n",
    "double click lower left corner to fill entire column\n",
    "make a list of fields (in excel datasheet) you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAME OF PROJECT\n",
    "\n",
    "#### S Sexton\n",
    "#### May 5, 2020\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [1. Project Goal](#project_goal)\n",
    "    * [Section 1.1](#section_1_1)\n",
    "        * [Section 1.2.1](#section_1_2_1)\n",
    "* [2. Background Information](#background_info)\n",
    "* [3. Acquire Data](#acquire)\n",
    "* [4. Load Libraries](#libraries)\n",
    "* [5. Set Working Directory](#directory)\n",
    "* [6. Load Dataset](#dataset)\n",
    "* [7. Cursory Exploration](#explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:    \n",
    "# must add and anchor to each title in table of contents - <a class=\"anchor\" id=\"project_goal\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github\n",
    "Upload to github everyday\n",
    "\n",
    "1. create repository\n",
    "2. make public\n",
    "3. initialize readme\n",
    "4. upload file\n",
    "5. drag and drop in file\n",
    "6. commit changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Project Goal <a class=\"anchor\" id=\"project_goal\"></a>\n",
    "- Create a summary of what the project goal is, information given, challenges, etc.\n",
    "- Understand the problem.\n",
    "- Maybe summarize the overall method being employed.\n",
    "\n",
    "KAGGLE PROJECTS:\n",
    "- Look at the description to get an idea of what you will need to do.\n",
    "- Look through the discussion section to get an idea of what the questions are to be answered.\n",
    "- The data section will explain what the headers mean.\n",
    "- Try to understand the problems by looking at web videos, researching the web.\n",
    "\n",
    "NON-KAGGLE PROJECTS:\n",
    "- Must get a clear idea of what the question is.\n",
    "- This may include interviewing who you are doing the project for to find out what they REALLY want.\n",
    "- What dataset do I need to investigate this quesion. What dataset do I have? Do I need to add to the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background Information<a class=\"anchor\" id=\"background_info\"></a>\n",
    "\n",
    "- list useful information that contributes to the understanding of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Acquire Data <a class=\"anchor\" id=\"acquire\"></a>\n",
    "\n",
    "- This may not always be necessary.\n",
    "- May need to scrape the web for this.\n",
    "- Add more here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initial Cleaning of Data <a class=\"anchor\" id=\"acquire\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load Libraries <a class=\"anchor\" id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Libraries that are needed and have not yet been installed\n",
    "- Ensure in correct directory or will not install properly\n",
    "- in juptyer: ! pip install (name of package here)\n",
    "\n",
    "#### Load libraries needed for the project\n",
    "- in juptyer: import pandas\n",
    "- can use alias for easier programming: import pandas as pd\n",
    "\n",
    "If only going to use a couple libraries from a module then import only those\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "\n",
    "If using magic functions, import those\n",
    "\n",
    "The following renders the figure in a notebook (instead of displaying a dump of the figure object)\n",
    "%matplotlib inline\n",
    "\n",
    "It's nice to say why you are using the library\n",
    "import seaborn as sns # for charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Common libraries are:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import os #change working directory\n",
    "sns.set_style('whitegrid')\n",
    "# sets plot inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add any additional calls for general control to data or output\n",
    "\n",
    "\n",
    "- suppresses repeated warnings from the same source to cut down on the annoyance of seeing the same message over and over\n",
    "- warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "- There are five preset seaborn themes: darkgrid, whitegrid, dark, white, and ticks. \n",
    "- They are each suited to different applications and personal preferences. The default theme is darkgrid.\n",
    "- sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "- pandas has an options system that lets you customize some aspects of its behaviour, \n",
    "- display-related options being those the user is most likely to adjust\n",
    "- the following displays a maximum of 100 columns\n",
    "- pd.set_option('max_columns', 100)\n",
    "\n",
    "\n",
    "- The following sets color palette. Named palettes default to 6 colors but the following sets it to 10 colors.\n",
    "- Gives a more diverse range for graphs\n",
    "- my_pal = sns.color_palette(n_colors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Set Working Directory <a class=\"anchor\" id=\"directory\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure in the correct working directory and change if necessary\n",
    "\n",
    "# get working directory\n",
    "os.getcwd()\n",
    "\n",
    "# change working directory \n",
    "os.chdir('C:\\\\Users\\\\sexto\\\\Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if manually downloading data, can check files in directory to ensure it is there\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Load Dataset <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIZE OF DATA IS IMPORTANT\n",
    "- Look at the SIZE of the dataset(s) (usually train). Look at when not compressed.\n",
    "- 4 GB is quite large. A GB is 1,000,000 KB.\n",
    "- In kaggle, click on grid icon and can see the size of the dataset if click on the file.\n",
    "- Otherwise, after downloading, click properties over the file name and can see the size.\n",
    "- If file is very large then must read in only part of it. There is a way to break this up like the earthquake file.\n",
    "- train = train.sample(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different packages have their own way of reading data. \n",
    "- Pandas\n",
    "- Dask\n",
    "- Datatable\n",
    "- Rapids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from methods of reading data from the raw csv files, it is also common to convert the dataset into another format which uses lesser disk space, is smaller in size and/or can be read faster for subsequent reads. The file types explored in the notebook (Default csv and rest alphabetically):\n",
    "- csv\n",
    "- feather\n",
    "- hdf5\n",
    "- jay\n",
    "- parquet\n",
    "- pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that just reading data is not the end of the story. The final decision of which method to use should also consider the downstream tasks and processes of the data that will be required to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PANDAS\n",
    "Pandas is probably the most popular method of reading datasets and is also the default on Kaggle. It has a lot of options, flexibility and functions for reading and processing data.\n",
    "\n",
    "One of the challenges with using pandas for reading large datasets is it's conservative nature while infering data types of the columns of a dataset often resulting in unnecessary large memory usage for the pandas dataframe. You can pre-define optimal data types of the columns (based on prior knowledge or sample inspection) and provide it explicitly while reading the dataset.\n",
    "\n",
    "Documentation: https://pandas.pydata.org/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%%time\n",
    "\n",
    "dtypes = {\n",
    "    \"row_id\": \"int64\",\n",
    "    \"user_id\": \"int32\",\n",
    "    \"task_container_id\": \"int16\",\n",
    "    \"user_answer\": \"int8\",\n",
    "    \"prior_question_elapsed_time\": \"float32\", \n",
    "    \"prior_question_had_explanation\": \"boolean\"\n",
    "}\n",
    "\n",
    "data = pd.read_csv(\"../input/riiid-test-answer-prediction/train.csv\", dtype=dtypes)\n",
    "\n",
    "print(\"Train size:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORT EXCEL SHEETS WITH PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT EXCEL SHEETS WITH PANDAS\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)\n",
    "\n",
    "# Load a sheet into a DataFrame by name of sheet\n",
    "df1 = xls.parse('2004')\n",
    "\n",
    "# Load a sheet into a DataFrame by index\n",
    "df2 = xls.parse(0)\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column\n",
    "df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DASK\n",
    "Dask provides a framework to scale pandas workflows natively using a parallel processing architecture. For those of you who have used Spark, you will find an uncanny similarity between the two.\n",
    "\n",
    "Documentation: https://docs.dask.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dtypes = {\n",
    "    \"row_id\": \"int64\",\n",
    "    \"user_id\": \"int32\",\n",
    "    \"task_container_id\": \"int16\",\n",
    "    \"user_answer\": \"int8\",\n",
    "    \"prior_question_elapsed_time\": \"float32\", \n",
    "    \"prior_question_had_explanation\": \"boolean\"\n",
    "}\n",
    "\n",
    "data = dd.read_csv(\"../input/riiid-test-answer-prediction/train.csv\", dtype=dtypes).compute()\n",
    "\n",
    "print(\"Train size:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATATABLE\n",
    "Datatable (heavily inspired by R's data.table) can read large datasets fairly quickly and is often faster than pandas. It is specifically meant for data processing of tabular datasets with emphasis on speed and support for large sized data.\n",
    "\n",
    "Documentation: https://datatable.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datatable installation with internet\n",
    "# !pip install datatable==0.11.0 > /dev/null\n",
    "\n",
    "# datatable installation without internet\n",
    "!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null\n",
    "\n",
    "import datatable as dt\n",
    "\n",
    "%%time\n",
    "\n",
    "data = dt.fread(\"../input/riiid-test-answer-prediction/train.csv\")\n",
    "\n",
    "print(\"Train size:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAPIDS\n",
    "Rapids is a great option to scale data processing on GPUs. With a lot of machine learning modelling moving to GPUs, Rapids enables to build end-to-end data science solutions on one or more GPUs.\n",
    "\n",
    "Documentation: https://docs.rapids.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rapids installation (make sure to turn on GPU)\n",
    "import sys\n",
    "!cp ../input/rapids/rapids.0.15.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n",
    "\n",
    "import cudf\n",
    "\n",
    "%%time\n",
    "\n",
    "data = cudf.read_csv(\"../input/riiid-test-answer-prediction/train.csv\")\n",
    "\n",
    "print(\"Train size:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RELATIONAL DATABASE\n",
    "A relational database is a type of database that stores and provides access to data points that are related to one another. ... The columns of the table hold attributes of the data, and each record usually has a value for each attribute, making it easy to establish the relationships among data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM Album\", engine)\n",
    "\n",
    "# Perform query and save results to DataFrame: df\n",
    "# Save dataframe column names to corresponding names of table columns\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SCRAPE DATA FROM WEBSITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# OR\n",
    "# read from website\n",
    "df = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.text\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can read it in with one of the above methods then convert.\n",
    "- eg. read in with fread then convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from csv using datatable and converting to pandas\n",
    "# data = dt.fread(\"../input/riiid-test-answer-prediction/train.csv\").to_pandas()\n",
    "\n",
    "# writing dataset as csv\n",
    "# data.to_csv(\"riiid_train.csv\", index=False)\n",
    "\n",
    "# writing dataset as hdf5\n",
    "# HDF5 is a high-performance data management suite to store, manage and process large and complex data.\n",
    "# data.to_hdf(\"riiid_train.h5\", \"riiid_train\")\n",
    "\n",
    "# writing dataset as feather\n",
    "# It is common to store data in feather (binary) format specifically for pandas. \n",
    "# It significantly improves reading speed of datasets.\n",
    "# data.to_feather(\"riiid_train.feather\")\n",
    "\n",
    "# writing dataset as parquet\n",
    "# Format: parquet\n",
    "# In the Hadoop ecosystem, parquet was popularly used as the primary file format for tabular datasets \n",
    "# and is now extensively used with Spark.\n",
    "# data.to_parquet(\"riiid_train.parquet\")\n",
    "\n",
    "# writing dataset as pickle\n",
    "# Python objects can be stored in the form of pickle files and pandas has inbuilt functions\n",
    "# to read and write dataframes as pickle objects.\n",
    "# data.to_pickle(\"riiid_train.pkl.gzip\")\n",
    "\n",
    "# writing dataset as jay\n",
    "# Datatable uses .jay (binary) format which makes reading datasets blazing fast. \n",
    "# dt.Frame(data).to_jay(\"riiid_train.jay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas requires a lot more RAM to handle large datasets.\n",
    "- Dask can be slow at times especially with transformations that cannot be parallelized.\n",
    "- Datatable doesn't have a very exhaustive set of data processing functions.\n",
    "- Rapids is not useful if you don't have a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAKE REPRODUCIBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make code as reproducible as possible by loading data from the direct link from the web\n",
    "trainUrl = \"https://www.kaggle.com/c/titanic/download/GQf0y8ebHO0C4JXscPPp%2Fversions%2FXkNkvXwqPPVG0Qt3MtQT%2Ffiles%2Ftrain.csv\"\n",
    "df2 = pd.read_csv(trainUrl, sep=',')\n",
    "# However, reading in this file caused problems. I am unsure why? It was supposed to be clean data\n",
    "# Got an answer from stackoverflow from below which is tried dropping rows with errors but decided not to do this\n",
    "train = pd.read_csv(\"https://www.kaggle.com/c/titanic/download/GQf0y8ebHO0C4JXscPPp%2Fversions%2FXkNkvXwqPPVG0Qt3MtQT%2Ffiles%2Ftrain.csv\", error_bad_lines=False)\n",
    "\n",
    "# The workaround is to download the file to the computer then load in the file and record where downloaded from\n",
    "# Data downloaded from https://www.kaggle.com/c/titanic/data\n",
    "# Setting header to None ensures headers are not imported in IF you dont want them\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get stock data from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(2006, 1, 1)\n",
    "end = datetime.datetime(2016, 1, 1)\n",
    "\n",
    "# Bank of America\n",
    "BAC = data.DataReader(\"BAC\", 'google', start, end)\n",
    "\n",
    "# Could also do this for a Panel Object\n",
    "df = data.DataReader(['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC'],'google', start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Cursory Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People seem to combine the the test and train dataset and do the manipulation on that. Is this what I should do then separate it each time I do exploration of one of the variables for the charts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test dataset by adding Survived column to test dataset and with NaN\n",
    "# Combine them so can clean data and feature engineer\n",
    "combined_df =  pd.concat(objs=[train_df, test_df], axis=0, sort=False).reset_index(drop=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines any duplicates based on all columns\n",
    "duplicateRowsDF = combined_df[combined_df.duplicated(keep='first')]\n",
    "duplicateRowsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Variable Type\n",
    "Need to look into this:\n",
    "categorical - nominal, ordinal, ratio, or interval\n",
    "numeric - discrete, continuous, or timeseries\n",
    "Among other things this helps us select the appropriate plots for visualization\n",
    "\n",
    "\n",
    "\n",
    "Python has variable types: \n",
    "    - numbers\n",
    "        - int (eg. 5, -6)\n",
    "        - long (eg. 51924361L, 0xDEFABCEC)\n",
    "        - float (eg. 15.20, 32.3+e18)\n",
    "        - complex (eg. 9.322e-36j)\n",
    "    - boolena (eg. True or False)\n",
    "    - strings (eg. HelloWorld!)\n",
    "    - list (eg. ['abcd', 786, 2.23, 'john', 70.2])\n",
    "    - tuple (eg. ( 'abcd', 786 , 2.23, 'john', 70.2  ) )\n",
    "    - dictionary (eg. {'name': 'john','code':6734, 'dept': 'sales'})\n",
    "\n",
    "If given in dataset, look at the given description of the column names (OR search the web to determine what the variable means) to determine if the type of variable listed in \"info\" is appropriate or should be changed.\n",
    " Need more info here on when to change and the variable types\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- from head & tail, determine type of data (categorical, numeric)\n",
    "- from describe, look at the min and max of each column to determine if there may be possible outliers\n",
    "- from info, determine the number of non-null entries\n",
    "- Within categorical features are the values nominal, ordinal, ratio, or interval based? Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- print(\"Number of Country_Region: \", train['Country_Region'].nunique())\n",
    "- print(\"Dates go from day\", max(train['Date']), \"to day\", min(train['Date']), \", a total of\", train['Date'].nunique(), \"days\")\n",
    "- print(\"Countries with Province/State informed: \", train.loc[train['Province_State']!='None']['Country_Region'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DETERMINE SIZE\n",
    "- Determine number of rows and columns in the dataframe (test and train)\n",
    "- This will help determine what columns may need to be added if train and test are merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size \n",
    "sizeTrain = train_df.size\n",
    "sizeTest = test_df.size\n",
    "  \n",
    "# shape \n",
    "shapeTrain = train_df.shape \n",
    "shapeTest = test_df.shape\n",
    "\n",
    "# printing size and shape \n",
    "print(\"SizeTrain = {}\\nShapeTrain = {}\".\n",
    "format(sizeTrain, shapeTrain))\n",
    "print(\"SizeTest = {}\\nShapeTest = {}\".\n",
    "format(sizeTest, shapeTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMMON STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head, gives first few rows of data\n",
    "# can change n number to get more rows\n",
    "df.head(n=5)\n",
    "\n",
    "# tail\n",
    "df.tail()\n",
    "\n",
    "# describe gives, count, mean, std, etc.\n",
    "df.describe()\n",
    "\n",
    "# info gives variables, types, nulls, etc.\n",
    "# change the type of column is needed\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type of the objects in the timeStamp column\n",
    "type(df['timeStamp'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TIME STAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING TIME STAMPS\n",
    "# Use pd.to_datetime to convert the column from strings to DateTime objects\n",
    "df['timeStamp'] = pd.to_datetime(df['timeStamp'])\n",
    "\n",
    "# create new columns\n",
    "df['Hour'] = df['timeStamp'].apply(lambda time: time.hour)\n",
    "df['Month'] = df['timeStamp'].apply(lambda time: time.month)\n",
    "df['Day of Week'] = df['timeStamp'].apply(lambda time: time.dayofweek)\n",
    "\n",
    "# Notice how the Day of Week is an integer 0-6. \n",
    "# Use the .map() with this dictionary to map the actual string names to the day of the week\n",
    "dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\n",
    "df['Day of Week'] = df['Day of Week'].map(dmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 zipcodes for 911 calls\n",
    "df['zip'].value_counts().head(5)\n",
    "\n",
    "# gives NUMBER of unique values in column specified\n",
    "df['col2'].nunique()\n",
    "\n",
    "# gives LIST of unique values in column specified\n",
    "df['col2'].unique()\n",
    "\n",
    "# gives dataframe of value AND count of that value\n",
    "df['col2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the max Close price for each bank's stock throughout the time period?\n",
    "# xs() function is used to get cross-section from the Series/DataFrame\n",
    "# this dataframe had bank stock header then a stock info ticker\n",
    "bank_stocks.xs(key='Close',axis=1,level='Stock Info').max()\n",
    "\n",
    "\n",
    "for tick in tickers:\n",
    "    returns[tick+' Return'] = bank_stocks[tick]['Close'].pct_change()\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATA TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers\n",
    "# eg. 1, 3, 111\n",
    "\n",
    "# Strings\n",
    "# can use single or double quotes\n",
    "# if use a quote inside of a quote, use this:\n",
    "\" wrap lot's of other quotes\"\n",
    "\n",
    "# Lists\n",
    "# square brackets\n",
    "# can be strings or numbers\n",
    "['hi',1,[1,2]]\n",
    "# indexing lists\n",
    "# get first element: my_list[0]\n",
    "# get all elements from 1 onwards: my_list[1:]\n",
    "\n",
    "# Dictionaries\n",
    "d = {'key1':'item1','key2':'item2'}\n",
    "\n",
    "# Booleans\n",
    "True or False\n",
    "\n",
    "# Tuples\n",
    "t = (1,2,3)\n",
    "\n",
    "# Sets\n",
    "{1,2,3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CHANGE DATA TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DEAL WITH MISSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is data missing at RANDOM?\n",
    "# shouldnt just drop rows if not random, better off flagging missing data\n",
    "\n",
    "# drops any rows with missing values\n",
    "# inside brackets, can set Thresh: Thresh=1 means that it keeps the columns which atleast contain 1 NON NA-value.\n",
    "df.dropna()\n",
    "\n",
    "# drops any columns with missing values\n",
    "df.dropna(axis=1)\n",
    "\n",
    "# fill na with fill value\n",
    "df.fillna(value='FILL VALUE')\n",
    "\n",
    "# fill with mean\n",
    "df['A'].fillna(value=df['A'].mean())\n",
    "\n",
    "# use feature engineering to fill in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean of a column that is grouped\n",
    "df.groupby('Company').mean()\n",
    "\n",
    "# can use: .std(), .min(), .max(), .count(), .transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JOINING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation \n",
    "# basically glues together DataFrames\n",
    "# Keep in mind that dimensions should match along the axis you are concatenating on\n",
    "# You can use pd.concat and pass in a list of DataFrames to concatenate together\n",
    "pd.concat([df1,df2,df3])\n",
    "\n",
    "# Merging\n",
    "# The merge function allows you to merge DataFrames together using a similar logic as merging SQL Tables together\n",
    "pd.merge(left, right, how='outer', on=['key1', 'key2'])\n",
    "\n",
    "# Joining is a convenient method for combining the columns of two potentially differently-indexed DataFrames into a single result\n",
    "left.join(right, how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the titles column there are \"Reasons/Departments\" specified before the title code. Separate it out (EMS: BACK PAINS/INJURY)\n",
    "df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data to determine: missing values, high/ low, inappropriate, type of values, categorical/ numerical, etc. see if some columns have too many missing values and possibly remove them, look for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times2(var):\n",
    "    return var*2\n",
    "\n",
    "# lambda functions\n",
    "lambda var: var*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Visualizations to understand the data in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to learn what kind of chart to use, the colors, probabily or number,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# After inspecting the data, merge similar datasets. However, don't merge train and test. Keep test set separate if possible\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\n",
    "# train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "\n",
    "# SET PROPER TYPE\n",
    "# Make sure the data in each column is the proper type.\n",
    "# This helps with memory management.\n",
    "# Data type will determine what values you can assign to it and what you can do to it (including operations you can perform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# put as a percentage\n",
    "\n",
    "# Dealing with missing values\n",
    "# Drop column if too many missing values.\n",
    "# inplace=True avoids assignment statement\n",
    "# train.drop('column_name', axis='columns', inplace=True)\n",
    "# dropna() drops row bases on the presence of missing values\n",
    "# dataset.dropna(subset=['col1', 'col2'], inplace=True)\n",
    "# ME: think i would like a table of all the columns and missing values (get code for this)\n",
    "\n",
    "# Classifying. We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n",
    "\n",
    "# Correlating. One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a correlation among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n",
    "\n",
    "# Completing. Data preparation may also require us to estimate any missing values within a feature. \n",
    "# - estimate, drop, proxy, other ways to fill in data\n",
    "\n",
    "# Correcting. We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n",
    "\n",
    "# Creating. Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n",
    "\n",
    "# When you know that a specific column will be critical to your analysis, and only a small fraction of rows are missing \n",
    "# a value in that column, it often makes sense to remove those rows from the dataset.\n",
    "\n",
    "# Ensure the columns have the proper data types\n",
    "# types are: object (python strings, lists), boolean (true and false values), int, float, datetime, category\n",
    "# best to not have data as strings\n",
    "# int - enables mathematical operations\n",
    "# float - enables mathematical operations\n",
    "# datetime - enables date-based attributes and methods\n",
    "# category - uses less memory and runs faster\n",
    "# bool - enable logiccal and mathematical operations\n",
    "\n",
    "# check datatypes as dataset.column.dtype\n",
    "# change datatype where the brackets on left side creates or overwrites existing series\n",
    "# dataset['column_name']=dataset.column.astype('data_type')\n",
    "\n",
    "\n",
    "# Should create scatterplots and box plots to look at data\n",
    "# Scatterplot matrix really good way of plotting and visualizing every variable against every other variable.\n",
    "\n",
    "# Some variables act as surogates for the other . eg ldl vs obesity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample\n",
    "- Do the problem on a small sample of data to ensure the results are as expected and then apply to the larger set. Can do visualizations in excel if it helps.\n",
    "- Will be experimenting with techniques, etc. and not all of these will produce the intended result but trying things is the key to getting the desired technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data\n",
    "- I personally don't believe in charting data for the sake of charting data. I see people just charting the data but it doesn't serve to help explain any of the questions.\n",
    "- Bar charts are used when:\n",
    "- Heat maps are used when: a graphical representation of data in which data values are represented as colors.\n",
    "display numeric data (eg. region on y axis and each product subcategory on x axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of how many instances for 0,1,2,3 in a particular group (accuracy_group in this case)\n",
    "# game_session is \n",
    "train_labels.groupby('accuracy_group')['game_session'].count() \\\n",
    "    .plot(kind='barh', figsize=(15, 5), title='Target (accuracy group)')\n",
    "plt.show()\n",
    "\n",
    "# Get a count in sns that shows the % value it represents overall\n",
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(y='accuracy_group',  data=train_labels)\n",
    "\n",
    "total = len(train_labels['accuracy_group'])\n",
    "for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n",
    "        x = p.get_x() + p.get_width() + 0.02\n",
    "        y = p.get_y() + p.get_height()/2\n",
    "        ax.annotate(percentage, (x, y))\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis - Rich people survive at higher rate than poor people\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to do random forest, \n",
    "# Convert to integers\n",
    "train_df['Sex'] = train_df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Variables To Keep\n",
    "- if use cross validation, must use it to narrow down the variables; NOT just randomly pick\n",
    "- 5 methods of building models: all-in, backward elimination, forward selection, bidirectional elimination, score comparison\n",
    "- All-in use when know they are the true predictors OR you have to use all OR preparing for backward elimination\n",
    "- Backward elimination. 1. select significance level to stay in the model (eg 5%); 2. fit model with all predictors; 3. Remove the predictor with highest variable IF it is above the significance level; 4. Fit the model without this variable; 5. Repeat until predictor with highest p value is below significance.\n",
    "- Forward selection. 1. select significance level for model; 2. fit all simple regression models and choose model with lowest p value; 3. Keep this variable and fit all possible models with one extra predictor added to the one(s) you already have; 4. Consider the predictor with the lowest p-value. If above significance, redo step 3; 5. stop when highest predictor is below significance level\n",
    "- Bidirectional elimination. 1. select significance level to stay in model; 2. perform forward selection (new variable must have p<significance to enter); 3. perform all step of backward elimination (old variable must have p<significance to stay); 4. stop when no new variable to enter or add\n",
    "- All possible models/ score comparision. 1. select a criterion of goodness of fit (eg. Akakie criterion); 2. construct all possible regression models; 3. select one with best criterion; 4. model is done\n",
    "- Exceptions: Even though the p-value may be above the signifance level, if it is only marginally above, then look at adjusted R squared of the entire model to see if should keep the variable. If R squared goes up, model is a better fit so keep the variable that is slightly above the significance level.\n",
    "- When referring to the coefficients, ensure referring to units properly (whether dollars, km, etc.). Can compare if on same scale.\n",
    "- Coefficients for each variable change as change the variables in the model.\n",
    "\n",
    "- Best to understand the variables because if just put all the variables in, there is a chance that it may show significance in formula.\n",
    "- for example, things that are not significant may be row number, surname (but may be indicator of something else)\n",
    "\n",
    "TRANSFORM VARIABLES\n",
    "- Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n",
    "- change transform by scale, square root of variable, apply power to variable, log of variable like 10 (when greater than zero)\n",
    "- can add 1 to variable or split variable into 2\n",
    "- try it out and see if improves model\n",
    "- want to remove the tranformed variable from the model to avoid correleated varaibles\n",
    "- divide one variable by another OR multiply one variable by another eg wealth = balance/age\n",
    "- if create a new variable, remove correlated variables\n",
    "- determine in multicolinearity (which don't want), gretl does automatically, unsure how otherwise. Take suspected colinearity out of the model and the variable becomes significant then likely was colinearity.\n",
    "- can transform the new variables as well\n",
    "- create a correlation matrix to determine correlations of variables where 1 is perfectly correlated and want to avoid this. Avoid over 0.7, correct. 0.5 to 0.3, look into it. Below 0.3 is low correlation. Correlation is bad because it can't find the correct correlation for the variables.\n",
    "- should not just base on accuracy rate since can be accuracy paradox where abandonment of one model increases accuracy rate\n",
    "- cumulative accuracy profile can plot models against each other so can see how much gain get from one over the other. Diagonal line at 45 degrees is the random model. If model is under the curve, very bad model.\n",
    "- to get accuracy ratio, look at % on y axis when take 50% on x axis. If value on y less than 60% than bad. If y value between 60% & 70% then poor. If y value between 70% & 80% then good. If above then very good. If between 90% & 100% then may be too good.\n",
    "\n",
    "\n",
    "\n",
    "HOW DO I DETERMINE SIGNIFICANCE LEVEL\n",
    "explain dummy variables to replace a variable (use one less when fitting the model)\n",
    "good way to understand is visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA or missing values\n",
    "- Remove -  In the case of a very large dataset with very few missing values, this approach could potentially work really well. However, if the missing values are in cases that are also otherwise statistically distinct, this method may seriously skew the predictive model for which this data is used. \n",
    "- REPLACE missing values with the mean/median value of the feature in which they occur - For imbalanced data, the median may be more appropriate, while for symmetrical and more normally distributed data, the mean could be a better choice.\n",
    "- Label encode NAs as another level of a categorical variable - works with tree-based models and other models if the feature can be numerically transformed (one-hot encoding, frequency encoding, etc.). This technique does not work well with logistic regression.\n",
    "- Run predictive models that impute the missing data. This should be done in conjunction with some kind of cross-validation scheme in order to avoid leakage. This can be very effective and can help with the final model.\n",
    "- Use the number of missing values in a given row to create a new engineered feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are they CATEGORICAL (can be nominal like m/f or ordinal like ordered as in small/med/large) \n",
    "# OR NUMERIC (can be discrete like whole numbers or continous like fractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, it isn't good to customize the data. However, if there is not a lot of training data such as was the case with the earthquake competition where there were only 16 training earthquakes, it may be necessary to remove some data in order to ensure the training model is not skewed.\n",
    "In the case of the earthquake file, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST AND TRAIN\n",
    "- break up data into test and train sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earthquake competition (given 16 earthquakes as train and small segments as test):\n",
    "- winner: the signal had a certain time-trend that caused some issues specifically on mean and quantile based features. To partly overcome this, we added a constant noise to each 150k segment (both in train and test) by calculating np.random.normal(0, 0.5, 150_000). Additionally, after noise addition, we subtracted the median of the segment.\n",
    "- i did a bar chart of the most common values. The values were all very small. So i subtracted out a range of the small values.\n",
    "- One of our best final LGB model only used four features: (i) number of peaks of at least support 2 on the denoised signal, (ii) 20% percentile on std of rolling window of size 50, (iii) 4th and (iv) 18th Mel-frequency cepstral coefficients mean. We sometimes used a few more features (like for the NN, see below) but they are usually very similar. Those 4 are decently uncorrelated between themselves, and add good diversity. For each feature we always only considered it if it has a p-value >0.05 on a KS statistic of train vs test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEAS\n",
    "- I like the idea of dropping the lowest and highest values\n",
    "- taking average, max,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times, people COMBINE SEVERAL MODELS together to get final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the MODE (OR VALUE THAT APPEARS MOST OFTEN) is a very simple method of prediction.\n",
    "# example: used the mode value of each Assessment task in the train_labels dataset and used them as predictions\n",
    "# value_counts() counts the number of values in each grouping & sort high to low AND index[0] returns first value in each count\n",
    "# nameofdictionary = dict(df_name.groupby('columnname1')[columnname2].agg(lambda x:x.value_counts().index[0]))\n",
    "# labels_map = dict(train_labels.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Algorithm\n",
    "- linear regression\n",
    "- polynomial regression\n",
    "- poisson regression\n",
    "- ordinary least squares regression (OLS)\n",
    "- ordinal regression\n",
    "- support vector regression\n",
    "- gradient descent regression\n",
    "- stepwise regression\n",
    "- lasso regression\n",
    "- ridge regression\n",
    "- elastic net regression\n",
    "- bayesian linear regression\n",
    "- least-angled regression (LARS)\n",
    "- neural network regression\n",
    "- locally estimated scatterplot smoothing (LOESS)\n",
    "- multivariate adaptive regression splines (MARS)\n",
    "- locally weighted regression (LWL)\n",
    "- quantile regression\n",
    "- principal component regression (PCR)\n",
    "- partial least squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear fit\n",
    "# in R, fit1=lm(medv~lstat, data=Boston)\n",
    "\n",
    "# Multiple linear regression\n",
    "# in R, fit2=lm(medv~lstat+age, data=Boston)\n",
    "# in R to fit all, fit3=lm(med~., Boston)\n",
    "# in R to remove a couple variables, fit4=update(fit3, ~.-age-indus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can get a summary of fit that shows residuals, co-efficients\n",
    "# in R, summary(nameofmodel)\n",
    "# in python, THINK nameof model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold Regression\n",
    "# Segmented regression, also known as piecewise regression or broken-stick regression\n",
    "# the independent variable is partitioned into intervals and a separate line segment is fit to each interval.\n",
    "# Segmented regression is useful when the independent variables, clustered into different groups, \n",
    "# exhibit different relationships between the variables in these regions.\n",
    "# the relations in the intervals are obtained by linear regression\n",
    "# my comments: instead of one straight line, there are two or more straight lines the have different slopes & intercepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohen's Kappa\n",
    "#  statistic that is used to measure inter-rater reliability (and also Intra-rater reliability) for qualitative (categorical) items\n",
    "# https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions\n",
    "- first line put def then the name of the function (square in this case) then () with parameter (value in this case) in the brackets:\n",
    "- second line must be indented then variable name is assigned to the function that want to perform on \"value\" parameter\n",
    "- third line will return the value from function\n",
    "- so when call function, pass argument 4 into function with square(4) and result is 16\n",
    "- def square(value):\n",
    "-     new_value = value ** 2\n",
    "-     return new_value\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
